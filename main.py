import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
import json

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://thediplomat.com/feed/",
"https://politepaul.com/fd/M8QukWfUPWR4.xml",
"https://politepaul.com/fd/BI4f9BiCvoed.xml",
    "https://www.foreignaffairs.com/rss.xml",
    "https://foreignpolicy.com/feed/",
    "https://evilgodfahim.github.io/ps/combined.xml",
    "https://evilgodfahim.github.io/eco/combined.xml",
    "https://www.defenseone.com/rss/all/",
    "https://www.spytalk.co/feed/",
    "https://rusi.org/rss/latest-publications.xml",
    "https://www.bellingcat.com/feed/",
    "https://www.thecipherbrief.com/feed"
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
LAST_SEEN_FILE = "last_seen.json"

MAX_ITEMS = 500
BD_OFFSET = 6  # Bangladesh UTC offset

# -----------------------------
# UTILITIES
# -----------------------------
def parse_date(entry):
    """Try multiple date fields from feedparser entry"""
    date_fields = ["published_parsed", "updated_parsed", "created_parsed"]
    for field in date_fields:
        t = getattr(entry, field, None)
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

def load_existing(file_path):
    """Load existing XML items"""
    if not os.path.exists(file_path):
        return []
    tree = ET.parse(file_path)
    root = tree.getroot()
    items = []
    for item in root.findall(".//item"):
        try:
            title = item.find("title").text or ""
            link = item.find("link").text or ""
            desc = item.find("description").text or ""
            pubDate = item.find("pubDate").text or ""
            pubDate_dt = datetime.strptime(pubDate, "%a, %d %b %Y %H:%M:%S %z")
            items.append({"title": title, "link": link, "description": desc, "pubDate": pubDate_dt})
        except:
            continue
    return items

def write_rss(items, file_path, title="Feed"):
    """Write items to RSS XML"""
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")
    ET.SubElement(channel, "title").text = title
    ET.SubElement(channel, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(channel, "description").text = f"{title} generated by script"

    for item in items:
        it = ET.SubElement(channel, "item")
        ET.SubElement(it, "title").text = item["title"]
        ET.SubElement(it, "link").text = item["link"]
        ET.SubElement(it, "description").text = item["description"]
        ET.SubElement(it, "pubDate").text = item["pubDate"].strftime("%a, %d %b %Y %H:%M:%S %z")

    # Pretty print
    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# MASTER FEED UPDATE
# -----------------------------
def update_master():
    print("[Updating feed_master.xml]")
    existing = load_existing(MASTER_FILE)
    existing_links = {x["link"] for x in existing}
    new_items = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = getattr(entry, "link", "")
                if link and link not in existing_links:
                    new_items.append({
                        "title": getattr(entry, "title", "No Title"),
                        "link": link,
                        "description": getattr(entry, "summary", ""),
                        "pubDate": parse_date(entry)
                    })
        except Exception as e:
            print(f"Error parsing {url}: {e}")

    all_items = existing + new_items
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    # Ensure at least one dummy item if empty
    if not all_items:
        all_items = [{
            "title": "No articles yet",
            "link": "https://evilgodfahim.github.io/",
            "description": "Master feed will populate after first successful fetch.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(all_items, MASTER_FILE, title="Master Feed (Updated every 30 mins)")
    print(f"✅ feed_master.xml updated with {len(all_items)} items")

# -----------------------------
# DAILY FEED UPDATE
# -----------------------------
def update_daily():
    print("[Updating daily_feed.xml]")
    to_zone = timezone(timedelta(hours=BD_OFFSET))

    # Load seen links
    seen_links_previous = set()
    if os.path.exists(LAST_SEEN_FILE):
        with open(LAST_SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            seen_links_previous = set(data.get("seen_links", []))

    # Calculate 48-hour cutoff in BD timezone
    now_bd = datetime.now(to_zone)
    cutoff_48h = now_bd - timedelta(hours=48)

    # Load master feed
    master_items = load_existing(MASTER_FILE)
    
    # Filter items: within 48 hours AND not previously seen
    new_items = []
    all_current_links = set()

    for item in master_items:
        pub = item["pubDate"].astimezone(to_zone)
        link = item["link"]
        
        # Track all links within 48 hours
        if pub > cutoff_48h:
            all_current_links.add(link)
            
            # Add to new_items if not previously seen
            if link not in seen_links_previous:
                new_items.append(item)

    # Sort by date (newest first)
    new_items.sort(key=lambda x: x["pubDate"], reverse=True)

    # Handle empty results
    if not new_items:
        new_items = [{
            "title": "No new articles in the last 48 hours",
            "link": "https://evilgodfahim.github.io/",
            "description": "Daily feed will populate when new articles appear.",
            "pubDate": datetime.now(timezone.utc)
        }]
        write_rss(new_items, DAILY_FILE, title="Daily Feed (Last 48 Hours, Updated 9 AM BD)")
        print(f"✅ daily_feed.xml updated - no new articles")
    else:
        # Write RSS feed
        write_rss(new_items, DAILY_FILE, title="Daily Feed (Last 48 Hours, Updated 9 AM BD)")
        print(f"✅ daily_feed.xml updated with {len(new_items)} new articles")

    # Save all current links within 48-hour window
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"seen_links": list(all_current_links)}, f, indent=2)

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()